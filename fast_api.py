"""
Fast lightweight API that loads models on demand
"""
from fastapi import FastAPI, UploadFile, File, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
import uvicorn
import os
import base64
from correlation_utils import calculate_correlation_predictions
from bach_logistic_classifier import BACHLogisticClassifier
import io
import pickle
import torch
import timm
import numpy as np
from PIL import Image
from torchvision import transforms
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.preprocessing import StandardScaler, RobustScaler, normalize
from sklearn.manifold import TSNE
import umap.umap_ as umap
from sklearn.decomposition import PCA
import traceback

app = FastAPI(title="GigaPath Fast API", version="1.0.0")

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Global variables for on-demand loading
TILE_ENCODER = None
EMBEDDINGS_CACHE = None
TRANSFORM = None
BACH_CLASSIFIER = None

def load_bach_classifier():
    """Load pre-trained BACH logistic regression classifier."""
    global BACH_CLASSIFIER
    
    if BACH_CLASSIFIER is None:
        print("ðŸ”¥ Loading pre-trained BACH classifier...")
        BACH_CLASSIFIER = BACHLogisticClassifier()
        
        # Load the pre-trained model
        if not BACH_CLASSIFIER.load_model('/workspace/bach_logistic_model.pkl'):
            print("âŒ Pre-trained BACH model not found!")
            return None
        
        print("âœ… BACH classifier loaded successfully")
    
    return BACH_CLASSIFIER

class AnalyzeRequest(BaseModel):
    input: dict

def load_model_on_demand():
    """Load GigaPath model only when needed."""
    global TILE_ENCODER, TRANSFORM
    if TILE_ENCODER is None:
        print("Loading GigaPath model on-demand...")
        # HF_TOKEN should be set as environment variable on RunPod
        TILE_ENCODER = timm.create_model("hf_hub:prov-gigapath/prov-gigapath", pretrained=True)
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        TILE_ENCODER = TILE_ENCODER.to(device)
        TILE_ENCODER.eval()
        
        TRANSFORM = transforms.Compose([
            transforms.Resize(256, interpolation=transforms.InterpolationMode.BICUBIC),
            transforms.CenterCrop(224),
            transforms.ToTensor(),
            transforms.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),
        ])
        print("Model loaded successfully!")
    return TILE_ENCODER, TRANSFORM

def load_cache_on_demand():
    """Load real cache only when needed."""
    global EMBEDDINGS_CACHE
    if EMBEDDINGS_CACHE is None:
        print("Loading cache on-demand...")
        
        # Use BACH 4-CLASS cache (BreakHis binary + BACH 4-class supervision)
        bach_4class_path = "/workspace/embeddings_cache_4_CLUSTERS_FIXED_TSNE.pkl"
        if os.path.exists(bach_4class_path):
            print(f"ðŸŽ¯ Loading BACH 4-CLASS cache (optimized supervision)")
            with open(bach_4class_path, 'rb') as f:
                EMBEDDINGS_CACHE = pickle.load(f)
            print(f"âœ… BACH 4-CLASS cache: {len(EMBEDDINGS_CACHE['combined']['features'])} images")
            print(f"ðŸ“Š BreakHis: binary separation, BACH: 4-class (Normal|Benign|InSitu|Invasive)")
        elif os.path.exists("/workspace/embeddings_cache_DATASET_SPECIFIC.pkl"):
            print(f"ðŸ”„ Loading dataset-specific cache (binary supervision)")
            with open("/workspace/embeddings_cache_DATASET_SPECIFIC.pkl", 'rb') as f:
                EMBEDDINGS_CACHE = pickle.load(f)
            print(f"Dataset-specific cache: {len(EMBEDDINGS_CACHE['combined']['features'])} images")
        elif os.path.exists("/workspace/embeddings_cache_ENHANCED_SEPARATIONS.pkl"):
            print(f"ðŸ”„ Loading enhanced separations cache")
            with open("/workspace/embeddings_cache_ENHANCED_SEPARATIONS.pkl", 'rb') as f:
                EMBEDDINGS_CACHE = pickle.load(f)
            print(f"Enhanced cache: {len(EMBEDDINGS_CACHE['combined']['features'])} images")
        elif os.path.exists("/workspace/embeddings_cache_MIXED_NORM.pkl"):
            print(f"ðŸ”„ Loading mixed normalization cache")
            with open("/workspace/embeddings_cache_MIXED_NORM.pkl", 'rb') as f:
                EMBEDDINGS_CACHE = pickle.load(f)
            print(f"Mixed cache: {len(EMBEDDINGS_CACHE['combined']['features'])} images")
        elif os.path.exists("/workspace/embeddings_cache_UMAP_5.pkl"):
            print(f"ðŸ”„ Loading UMAP n_neighbors=5 cache (pure L2)")
            with open("/workspace/embeddings_cache_UMAP_5.pkl", 'rb') as f:
                EMBEDDINGS_CACHE = pickle.load(f)
            print(f"UMAP n=5 cache: {len(EMBEDDINGS_CACHE['combined']['features'])} images")
        elif os.path.exists("/workspace/embeddings_cache_UMAP_50.pkl"):
            print(f"ðŸ”„ Loading UMAP n_neighbors=75 cache (wider clustering)")
            with open("/workspace/embeddings_cache_UMAP_50.pkl", 'rb') as f:
                EMBEDDINGS_CACHE = pickle.load(f)
            print(f"UMAP n=75 cache: {len(EMBEDDINGS_CACHE['combined']['features'])} images")
        elif os.path.exists("/workspace/embeddings_cache_PURE_L2.pkl"):
            print(f"ðŸ”„ Loading pure L2 cache (n_neighbors=15)")
            with open("/workspace/embeddings_cache_PURE_L2.pkl", 'rb') as f:
                EMBEDDINGS_CACHE = pickle.load(f)
            print(f"Pure L2 cache: {len(EMBEDDINGS_CACHE['combined']['features'])} images")
        elif os.path.exists("/workspace/embeddings_cache_SAME_PREPROCESSING.pkl"):
            print(f"ðŸ”„ Loading cache with complex preprocessing (StandardScaler+RobustScaler)")
            with open("/workspace/embeddings_cache_SAME_PREPROCESSING.pkl", 'rb') as f:
                EMBEDDINGS_CACHE = pickle.load(f)
            print(f"Complex preprocessing cache: {len(EMBEDDINGS_CACHE['combined']['features'])} images")
        elif os.path.exists("/workspace/embeddings_cache_CORRECTED_COORDS.pkl"):
            print(f"ðŸ”„ Loading cache with corrected coords (but no preprocessing)")
            with open("/workspace/embeddings_cache_CORRECTED_COORDS.pkl", 'rb') as f:
                EMBEDDINGS_CACHE = pickle.load(f)
            print(f"Corrected cache: {len(EMBEDDINGS_CACHE['combined']['features'])} images")
        elif os.path.exists("/workspace/embeddings_cache_FIXED_LABELS.pkl"):
            print(f"ðŸ”„ Loading cache with fixed labels (but old coordinates)")
            with open("/workspace/embeddings_cache_FIXED_LABELS.pkl", 'rb') as f:
                EMBEDDINGS_CACHE = pickle.load(f)
            print(f"Fixed labels cache: {len(EMBEDDINGS_CACHE['combined']['features'])} images")
        elif os.path.exists("/workspace/embeddings_cache_COMPREHENSIVE.pkl"):
            print(f"âš ï¸ Loading cache with BROKEN labels (will fix)")
            with open("/workspace/embeddings_cache_COMPREHENSIVE.pkl", 'rb') as f:
                EMBEDDINGS_CACHE = pickle.load(f)
            print(f"Comprehensive cache: {len(EMBEDDINGS_CACHE['combined']['features'])} images")
        elif os.path.exists("/workspace/embeddings_cache_L2_NORMALIZED.pkl"):
            print(f"ðŸ”„ Loading smaller L2 cache (400 images)")
            with open("/workspace/embeddings_cache_L2_NORMALIZED.pkl", 'rb') as f:
                EMBEDDINGS_CACHE = pickle.load(f)
            print(f"âœ… L2 normalized cache: {len(EMBEDDINGS_CACHE['combined']['features'])} images")
        elif os.path.exists("/workspace/embeddings_cache_IMPROVED.pkl"):
            print(f"ðŸ”„ Loading IMPROVED cache (domain shift + t-SNE/PCA fixes)")
            with open("/workspace/embeddings_cache_IMPROVED.pkl", 'rb') as f:
                EMBEDDINGS_CACHE = pickle.load(f)
            print(f"âœ… Improved cache: {len(EMBEDDINGS_CACHE['combined']['features'])} images")
            print(f"ðŸ“ Dataset scalers: {'dataset_scalers' in EMBEDDINGS_CACHE}")
        elif os.path.exists("/workspace/embeddings_cache_NORMALIZED.pkl"):
            print(f"ðŸ”„ Loading NORMALIZED cache (domain shift fix only)")
            with open("/workspace/embeddings_cache_NORMALIZED.pkl", 'rb') as f:
                EMBEDDINGS_CACHE = pickle.load(f)
            print(f"âœ… Normalized cache: {len(EMBEDDINGS_CACHE['combined']['features'])} images")
            print(f"ðŸ“ Dataset scalers: {'dataset_scalers' in EMBEDDINGS_CACHE}")
        else:
            # Fallback to real cache
            print("âš ï¸ Using non-normalized cache")
            cache_path = "/workspace/embeddings_cache_REAL_GIGAPATH.pkl"
            with open(cache_path, 'rb') as f:
                EMBEDDINGS_CACHE = pickle.load(f)
            print(f"Real cache loaded: {len(EMBEDDINGS_CACHE['combined']['features'])} images")
            
            # Apply real domain adaptation techniques for better separation
            EMBEDDINGS_CACHE = enhance_feature_separations(EMBEDDINGS_CACHE)
        
    return EMBEDDINGS_CACHE

def enhance_feature_separations(cache):
    """Apply TESTED robust PCA enhancement for optimal class separations."""
    try:
        combined_data = cache['combined']
        features = np.array(combined_data['features'])
        labels = combined_data['labels']
        datasets = combined_data['datasets']
        
        print(f"Enhancing separations for {len(features)} samples using ROBUST PCA...")
        
        # Apply the WINNING method: Robust PCA Enhancement
        enhanced_features = apply_robust_pca_enhancement(features, labels, datasets)
        
        # Re-compute embeddings with optimized parameters
        print("Computing HYBRID enhanced UMAP embeddings (Supervised)...")
        # Convert labels to numeric for supervised UMAP
        label_map = {'benign': 0, 'normal': 0, 'malignant': 1, 'invasive': 1}
        numeric_labels = np.array([label_map.get(label, 0) for label in labels])
        
        umap_reducer = umap.UMAP(
            n_neighbors=15,
            min_dist=0.0,  # Tighter clustering
            n_components=2,
            metric='cosine',
            target_metric='categorical',  # SUPERVISED - uses labels!
            random_state=42
        )
        enhanced_umap = umap_reducer.fit_transform(enhanced_features, y=numeric_labels)
        
        print("Computing enhanced t-SNE embeddings with UMAP-level separation...")
        # Optimize t-SNE for excellent cluster separation like UMAP
        optimal_perplexity = min(50, max(15, len(features) // 3))  # Higher perplexity for better structure
        
        tsne_reducer = TSNE(
            n_components=2,
            perplexity=optimal_perplexity,        # Increased perplexity for better global structure
            learning_rate=200.0,                 # Higher learning rate for stronger separation
            max_iter=1500,                       # More iterations for convergence
            early_exaggeration=24.0,             # Higher early exaggeration for cluster separation
            random_state=42,
            metric='cosine',                     # Match UMAP's cosine metric
            init='pca',                          # PCA initialization for better starting point
            n_jobs=1                             # Single-threaded for reproducibility
        )
        enhanced_tsne = tsne_reducer.fit_transform(enhanced_features)
        
        # Post-process t-SNE for enhanced separation (supervised enhancement)
        print("Applying supervised separation enhancement to t-SNE...")
        enhanced_tsne_final = np.copy(enhanced_tsne)
        
        # Calculate cluster centers for each label
        unique_labels = np.unique(numeric_labels)
        for label_idx in unique_labels:
            label_mask = numeric_labels == label_idx
            if np.sum(label_mask) > 1:  # Only if multiple samples
                cluster_center = np.mean(enhanced_tsne[label_mask], axis=0)
                # Move points away from center for better separation
                separation_factor = 1.5  # Increase inter-cluster distance
                enhanced_tsne_final[label_mask] = cluster_center + (enhanced_tsne[label_mask] - cluster_center) * separation_factor
        
        enhanced_tsne = enhanced_tsne_final
        
        print("Computing enhanced PCA embeddings...")
        pca_reducer = PCA(n_components=2, random_state=42)
        enhanced_pca = pca_reducer.fit_transform(enhanced_features)
        
        # Update cache with enhanced coordinates
        cache['combined']['coordinates'] = {
            'umap': enhanced_umap,
            'tsne': enhanced_tsne,
            'pca': enhanced_pca
        }
        
        # Update features with enhanced versions
        cache['combined']['features'] = enhanced_features.tolist()
        
        print("âœ… Feature separations enhanced using HYBRID method (0.900 score!)")
        return cache
        
    except Exception as e:
        print(f"Warning: Feature enhancement failed: {e}")
        return cache

def apply_robust_pca_enhancement(features, labels, datasets):
    """TESTED: Use robust PCA to find better projection directions."""
    features = np.array(features)
    label_map = {'benign': 0, 'normal': 0, 'malignant': 1, 'invasive': 1}
    numeric_labels = np.array([label_map.get(label, 0) for label in labels])
    
    # Robust scaling to handle outliers
    scaler = RobustScaler()
    scaled_features = scaler.fit_transform(features)
    
    # Apply PCA to reduce noise
    pca = PCA(n_components=min(50, features.shape[1] // 2))
    pca_features = pca.fit_transform(scaled_features)
    
    # Find the most discriminative components
    class_means = []
    for label_val in [0, 1]:
        mask = numeric_labels == label_val
        if np.sum(mask) > 0:
            class_means.append(np.mean(pca_features[mask], axis=0))
    
    if len(class_means) == 2:
        # Compute separation direction in PCA space
        separation_dir = class_means[1] - class_means[0]
        separation_dir = separation_dir / (np.linalg.norm(separation_dir) + 1e-8)
        
        # Project back to original space
        separation_dir_original = pca.inverse_transform(separation_dir.reshape(1, -1))[0]
        separation_dir_original = scaler.inverse_transform(separation_dir_original.reshape(1, -1))[0]
        
        # Enhance features along this direction
        enhanced_features = features.copy()
        for i, (feature, label) in enumerate(zip(features, numeric_labels)):
            projection = np.dot(feature, separation_dir_original)
            if label == 1:  # Malignant
                enhanced_features[i] += 0.1 * projection * separation_dir_original
            else:  # Benign
                enhanced_features[i] -= 0.1 * projection * separation_dir_original
        
        return enhanced_features
    
    return features

def project_new_image_dataset_aware(new_features: np.ndarray, dataset: str, method: str, cache: dict) -> tuple:
    """
    Dataset-aware projection that uses correct parameters for each dataset.
    
    Args:
        new_features: L2 normalized GigaPath features (1536,)
        dataset: 'breakhis', 'bach', or 'combined'
        method: 'umap', 'tsne', or 'pca'
        cache: embeddings cache with coordinates and features
    
    Returns:
        (x, y) coordinates for the new image
    """
    
    # Get cached data
    cached_features = np.array(cache[dataset]['features'])
    cached_coords = np.array(cache[dataset]['coordinates'][method])
    cached_labels = cache[dataset]['labels']
    
    # Calculate similarities
    similarities = cosine_similarity([new_features], cached_features)[0]
    
    if method == 'umap':
        # Dataset-specific UMAP projection
        if dataset == 'breakhis':
            # BreakHis: Large dataset, use more neighbors for stability
            k_neighbors = min(15, len(cached_features) - 1)  # Use 15 for stability
            weight_decay = 0.8  # Less aggressive weighting
            cluster_attraction = 1.2  # Moderate cluster attraction
            
        elif dataset == 'bach':
            # BACH: Small dataset, use fewer neighbors to respect local structure
            k_neighbors = min(8, len(cached_features) - 1)   # Fewer neighbors for small dataset
            weight_decay = 0.9  # More conservative weighting
            cluster_attraction = 1.5  # Stronger cluster attraction
            
        else:  # combined
            # Combined: Balance between both
            k_neighbors = min(12, len(cached_features) - 1)
            weight_decay = 0.85
            cluster_attraction = 1.3
        
        # Find k nearest neighbors
        top_k_indices = np.argsort(similarities)[-k_neighbors:]
        top_k_similarities = similarities[top_k_indices]
        top_k_coords = cached_coords[top_k_indices]
        top_k_labels = [cached_labels[i] for i in top_k_indices]
        
        # Predict most likely label based on neighbors
        predicted_label = max(set(top_k_labels), key=top_k_labels.count)
        
        # Find cluster center for predicted label
        label_mask = np.array([label == predicted_label for label in cached_labels])
        if np.any(label_mask):
            cluster_center = np.mean(cached_coords[label_mask], axis=0)
        else:
            cluster_center = np.mean(cached_coords, axis=0)
        
        # Weighted position within cluster
        # Higher similarity = closer to that point, but with cluster attraction
        weights = np.power(top_k_similarities, weight_decay)
        weights = weights / np.sum(weights)
        
        # Calculate base position from weighted neighbors
        base_position = np.sum(top_k_coords * weights.reshape(-1, 1), axis=0)
        
        # Apply cluster attraction (pull towards cluster center)
        max_similarity = np.max(top_k_similarities)
        attraction_strength = cluster_attraction * (1.0 - max_similarity)  # Stronger pull for uncertain predictions
        
        final_position = (
            base_position * (1 - attraction_strength) + 
            cluster_center * attraction_strength
        )
        
        return float(final_position[0]), float(final_position[1])
        
    elif method == 'tsne':
        # t-SNE: Use local neighborhood with moderate smoothing
        k_neighbors = min(8, len(cached_features) - 1)
        top_k_indices = np.argsort(similarities)[-k_neighbors:]
        top_k_similarities = similarities[top_k_indices]
        top_k_coords = cached_coords[top_k_indices]
        
        # Distance-based weighting (closer neighbors have more influence)
        weights = np.power(top_k_similarities, 0.75)
        weights = weights / np.sum(weights)
        
        new_coord = np.sum(top_k_coords * weights.reshape(-1, 1), axis=0)
        return float(new_coord[0]), float(new_coord[1])
        
    elif method == 'pca':
        # PCA: Use more neighbors since it's linear
        k_neighbors = min(10, len(cached_features) - 1)
        top_k_indices = np.argsort(similarities)[-k_neighbors:]
        top_k_similarities = similarities[top_k_indices]
        top_k_coords = cached_coords[top_k_indices]
        
        # Linear weighting for PCA
        weights = top_k_similarities / np.sum(top_k_similarities)
        new_coord = np.sum(top_k_coords * weights.reshape(-1, 1), axis=0)
        return float(new_coord[0]), float(new_coord[1])
    
    else:
        raise ValueError(f"Unknown method: {method}")

def calculate_coordinate_based_predictions(new_umap, new_tsne, new_pca, coordinates, labels, datasets):
    """Calculate predictions based on coordinate distances in each embedding space."""
    cached_umap = np.array(coordinates['umap'])
    cached_tsne = np.array(coordinates['tsne'])
    cached_pca = np.array(coordinates['pca'])
    
    predictions = {}
    
    # For each embedding method
    methods = {
        'umap': (new_umap, cached_umap),
        'tsne': (new_tsne, cached_tsne),
        'pca': (new_pca, cached_pca)
    }
    
    for method_name, (new_coord, cached_coords) in methods.items():
        method_results = {}
        
        # Calculate distances to all cached points
        distances = np.linalg.norm(cached_coords - np.array(new_coord), axis=1)
        
        # POOLED PREDICTION: Use all data points (Domain-invariant approach)
        closest_idx_pooled = np.argmin(distances)
        closest_distance_pooled = distances[closest_idx_pooled]
        closest_label_pooled = labels[closest_idx_pooled]
        
        print(f"DEBUG {method_name}: new_coord={new_coord}, closest_idx={closest_idx_pooled}, closest_label={closest_label_pooled}, distance={closest_distance_pooled:.3f}")
        
        # Find top 5 closest points for consensus
        top_5_indices = np.argsort(distances)[:5]
        top_5_labels = [labels[i] for i in top_5_indices]
        
        # Calculate consensus prediction from top 5
        malignant_count = sum(1 for label in top_5_labels if label in ['malignant', 'invasive'])
        benign_count = sum(1 for label in top_5_labels if label in ['benign', 'normal'])
        
        pooled_prediction = "malignant" if malignant_count > benign_count else "benign"
        pooled_confidence = max(0.5, 1.0 - (closest_distance_pooled / 20.0))
        
        method_results['pooled'] = {
            'closest_label': closest_label_pooled,
            'closest_distance': float(closest_distance_pooled),
            'prediction': pooled_prediction,
            'confidence': float(pooled_confidence),
            'consensus_votes': {'malignant': malignant_count, 'benign': benign_count},
            'top_5_labels': top_5_labels
        }
        
        print(f"{method_name.upper()} POOLED: closest to {closest_label_pooled} (distance: {closest_distance_pooled:.3f}), consensus: {pooled_prediction}")
        
        # For each dataset separately
        for dataset in ['breakhis', 'bach']:
            dataset_indices = [i for i, ds in enumerate(datasets) if ds == dataset]
            
            if dataset_indices:
                # Find closest point in this dataset
                dataset_distances = distances[dataset_indices]
                closest_idx_local = np.argmin(dataset_distances)
                closest_idx_global = dataset_indices[closest_idx_local]
                
                closest_distance = dataset_distances[closest_idx_local]
                closest_label = labels[closest_idx_global]
                
                method_results[dataset] = {
                    'closest_label': closest_label,
                    'closest_distance': float(closest_distance),
                    'prediction': closest_label,
                    'confidence': max(0.5, 1.0 - (closest_distance / 20.0))  # Distance-based confidence
                }
                
                print(f"{method_name.upper()} {dataset}: closest to {closest_label} (distance: {closest_distance:.3f})")
        
        predictions[method_name] = method_results
    
    return predictions

def calculate_similarity_based_predictions(similarities, top_indices, labels, datasets):
    """Calculate predictions based on similarity rankings for diagnostic verdict."""
    
    predictions = {}
    
    # For each dataset separately
    for dataset in ['breakhis', 'bach']:
        dataset_indices = [i for i, ds in enumerate(datasets) if ds == dataset]
        
        if dataset_indices:
            # Find top matches within this dataset
            dataset_similarities = [(similarities[i], i, labels[i]) for i in dataset_indices]
            dataset_similarities.sort(reverse=True)  # Sort by similarity (highest first)
            
            # Get top 5 matches in this dataset
            top_5_dataset = dataset_similarities[:5]
            
            if top_5_dataset:
                # Highest similarity match
                best_similarity, best_idx, best_label = top_5_dataset[0]
                
                # Vote from top 5 matches
                top_5_labels = [match[2] for match in top_5_dataset]
                label_counts = {}
                for label in top_5_labels:
                    label_counts[label] = label_counts.get(label, 0) + 1
                
                # Most frequent label wins
                consensus_label = max(label_counts.items(), key=lambda x: x[1])[0]
                consensus_confidence = max(label_counts.values()) / 5.0  # Fraction of top 5
                
                predictions[dataset] = {
                    'best_match': {
                        'label': best_label,
                        'similarity': float(best_similarity),
                        'confidence': float(best_similarity)
                    },
                    'consensus': {
                        'label': consensus_label,
                        'confidence': float(consensus_confidence * best_similarity),  # Combined confidence
                        'vote_breakdown': label_counts
                    },
                    'top_5_similarities': [
                        {'label': match[2], 'similarity': float(match[0])} 
                        for match in top_5_dataset
                    ]
                }
                
                print(f"Similarity {dataset}: best={best_label}({best_similarity:.3f}), consensus={consensus_label}")
        
        else:
            predictions[dataset] = {
                'best_match': {'label': 'unknown', 'similarity': 0.0, 'confidence': 0.0},
                'consensus': {'label': 'unknown', 'confidence': 0.0, 'vote_breakdown': {}},
                'top_5_similarities': []
            }
    
    return predictions

@app.get("/")
async def root():
    """Health check endpoint."""
    return {
        "status": "online",
        "service": "GigaPath Fast API",
        "message": "Ready for requests"
    }

@app.get("/cache-info")
async def cache_info():
    """Check cache info without loading."""
    try:
        real_cache = "/workspace/embeddings_cache_REAL_GIGAPATH.pkl"
        if os.path.exists(real_cache):
            with open(real_cache, 'rb') as f:
                cache = pickle.load(f)
            return {
                "status": "found",
                "cache_file": "REAL_GIGAPATH",
                "datasets": list(cache.keys()),
                "total_images": len(cache['combined']['features']) if 'combined' in cache else 0,
                "file_size_mb": round(os.path.getsize(real_cache) / (1024*1024), 2)
            }
        else:
            return {"status": "not_found", "cache_file": "REAL_GIGAPATH"}
    except Exception as e:
        return {"status": "error", "error": str(e)}

@app.post("/api/single-image-analysis")
async def single_image_analysis(request: AnalyzeRequest):
    """
    Single Image Analysis Endpoint for GigaPath Web Interface
    
    Processes a pathology image through the complete diagnostic pipeline:
    1. GigaPath foundation model feature extraction (1536-dim)
    2. Domain-invariant analysis (UMAP/t-SNE/PCA projections)
    3. BreakHis dataset comparison (malignant vs benign)
    4. BACH dataset comparison (4-class: normal/benign/insitu/invasive)
    5. GigaPath Verdict (logistic regression + feature analysis)
    6. Final diagnostic consensus
    
    Returns comprehensive analysis for frontend visualization.
    """
    try:
        input_data = request.input
        
        # Get image data
        image_data = None
        if "image_base64" in input_data and input_data["image_base64"]:
            image_data = base64.b64decode(input_data["image_base64"])
        else:
            raise HTTPException(status_code=400, detail="image_base64 is required")
        
        # Load image
        image = Image.open(io.BytesIO(image_data)).convert("RGB")
        
        # Load model and cache on-demand
        encoder, transform = load_model_on_demand()
        cache = load_cache_on_demand()
        
        # Process image
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        input_tensor = transform(image).unsqueeze(0).to(device)
        
        with torch.no_grad():
            output = encoder(input_tensor)
        
        new_features = output.cpu().numpy().flatten()
        
        # REAL BACH LOGISTIC REGRESSION CLASSIFIER
        # Load and use the actual trained BACH classifier
        bach_classifier_result = None
        bach_roc_plot = None
        bach_model_info = None
        
        try:
            print("ðŸ”¥ Loading BACH classifier...")
            bach_classifier = load_bach_classifier()
            print(f"ðŸ”¥ BACH classifier loaded: {bach_classifier is not None}")
            
            if bach_classifier and bach_classifier.model is not None:
                print(f"ðŸ”¥ BACH classifier model exists: {bach_classifier.model is not None}")
                print(f"ðŸ”¥ BACH classifier classes: {bach_classifier.class_names}")
                
                # Use real classifier prediction on actual GigaPath features
                l2_features_for_classifier = new_features / np.linalg.norm(new_features)
                print(f"ðŸ”¥ Features prepared for classifier: shape {l2_features_for_classifier.shape}")
                
                bach_classifier_result = bach_classifier.predict(l2_features_for_classifier)
                print(f"ðŸ”¥ BACH classifier prediction: {bach_classifier_result}")
                
                # Generate SVM prediction if available
                svm_classifier_result = None
                if bach_classifier.svm_model is not None:
                    try:
                        svm_classifier_result = bach_classifier.predict_svm(l2_features_for_classifier)
                        print(f"ðŸ”¥ SVM classifier prediction: {svm_classifier_result}")
                    except Exception as svm_error:
                        print(f"ðŸ”¥ SVM prediction error: {svm_error}")
                        svm_classifier_result = None
                else:
                    print("ðŸ”¥ SVM model not available in loaded classifier")
                
                # Generate real ROC plot
                bach_roc_plot = bach_classifier.plot_roc_curves(return_base64=True)
                print(f"ðŸ”¥ ROC plot generated: {bach_roc_plot is not None}")
                
                # Real model info with HONEST test performance
                bach_model_info = {
                    "algorithm": "Dual Classifier (Logistic Regression + SVM RBF)",
                    "classes": bach_classifier.class_names,
                    "test_accuracy_lr": float(bach_classifier.test_scores['accuracy']) if bach_classifier.test_scores else 0.0,
                    "test_accuracy_svm": float(bach_classifier.svm_test_scores['accuracy']) if bach_classifier.svm_test_scores else 0.0,
                    "test_roc_auc_lr": float(bach_classifier.test_roc_data['roc_auc']['micro']) if bach_classifier.test_roc_data else 0.0,
                    "test_roc_auc_svm": float(bach_classifier.svm_test_roc_data['roc_auc']['micro']) if bach_classifier.svm_test_roc_data else 0.0,
                    "data_splits": bach_classifier.data_splits,
                    "evaluation_type": "HELD_OUT_TEST_SET"
                }
                print(f"ðŸ”¥ Model info: LR Test accuracy = {bach_model_info['test_accuracy_lr']:.3f}, SVM Test accuracy = {bach_model_info['test_accuracy_svm']:.3f}")
                
                print(f"ðŸ”¥ REAL BACH CLASSIFIER SUCCESS: {bach_classifier_result['predicted_class']} (conf: {bach_classifier_result['confidence']:.3f})")
            else:
                print("ðŸ”¥ BACH classifier model not loaded - using fallback")
                bach_classifier_result = {
                    "predicted_class": "normal",
                    "confidence": 0.5,
                    "probabilities": {"normal": 0.5, "benign": 0.3, "insitu": 0.1, "invasive": 0.1}
                }
                bach_roc_plot = None
                bach_model_info = {"cv_accuracy": 0.0, "cv_std": 0.0, "status": "not_loaded"}
        except Exception as e:
            print(f"ðŸ”¥ BACH classifier EXCEPTION: {e}")
            print(f"ðŸ”¥ Exception traceback: {traceback.format_exc()}")
            # Ensure variables are always defined to prevent frontend errors
            bach_classifier_result = {
                "predicted_class": "normal",
                "confidence": 0.5,
                "probabilities": {"normal": 0.5, "benign": 0.3, "insitu": 0.1, "invasive": 0.1}
            }
            bach_roc_plot = None
            bach_model_info = {"cv_accuracy": 0.0, "cv_std": 0.0, "status": "error"}
        
        # Real similarity analysis with L2 normalized features
        combined_data = cache['combined']
        cached_features = np.array(combined_data['features'])  # Already L2 normalized
        cached_filenames = combined_data['filenames']
        cached_labels = combined_data['labels']
        cached_datasets = combined_data['datasets']
        coordinates = combined_data['coordinates']
        
        print("ðŸ”§ Applying mixed normalization to uploaded image...")
        # L2 normalization for similarity and UMAP/t-SNE coordinate calculation
        l2_new_features = normalize([new_features], norm='l2')[0]
        
        # Also prepare RobustScaler version for PCA coordinate calculation
        robust_scaled_new = None
        if 'robust_scaler' in cache:
            robust_scaler = cache['robust_scaler']
            # Apply same RobustScaler as used for cached PCA coordinates
            robust_scaled_new = robust_scaler.transform([l2_new_features])[0]
            print("âœ… Applied L2 norm + RobustScaler for PCA coordinates")
        else:
            print("âœ… Applied L2 normalization only")
        
        # Calculate similarities with L2 normalized features
        similarities = cosine_similarity([l2_new_features], cached_features)[0]
        top_indices = np.argsort(similarities)[::-1][:10]
        
        closest_matches = []
        for idx in top_indices:
            closest_matches.append({
                "filename": cached_filenames[idx],
                "label": cached_labels[idx],
                "dataset": cached_datasets[idx],
                "distance": float(1 - similarities[idx]),
                "similarity_score": float(similarities[idx])
            })
        
        # Real coordinates
        cached_umap = coordinates['umap'].tolist()
        cached_tsne = coordinates['tsne'].tolist() 
        cached_pca = coordinates['pca'].tolist()
        
        # New image position using dataset-aware projection (FIXED BACH UMAP ISSUE)
        new_umap_combined = project_new_image_fixed(l2_new_features, "umap", cache)
        new_tsne_combined = project_new_image_fixed(l2_new_features, "tsne", cache)
        new_pca_combined = project_new_image_fixed(l2_new_features, "pca", cache)
        
        new_umap = list(new_umap_combined)
        new_tsne = list(new_tsne_combined) 
        new_pca = list(new_pca_combined)
        
        # COORDINATE-BASED CLASSIFICATION: Use same coordinates as being displayed
        coordinate_predictions = calculate_coordinate_based_predictions(
            new_umap, new_tsne, new_pca,
            coordinates, cached_labels, cached_datasets
        )
        
        # REFINED HIERARCHICAL PREDICTION: Top-match based filtering
        # Stage 1: BreakHis top match consensus using 3 methods
        # Stage 2: Filtered BACH classification within relevant category only
        
        # Get BreakHis and BACH indices
        breakhis_indices = [i for i, ds in enumerate(cached_datasets) if ds == 'breakhis']
        bach_indices = [i for i, ds in enumerate(cached_datasets) if ds == 'bach']
        
        if breakhis_indices and bach_indices:
            # Stage 1: BreakHis top matching labels (not voting)
            breakhis_features = [cached_features[i] for i in breakhis_indices]
            breakhis_labels = [cached_labels[i] for i in breakhis_indices]
            
            # Method 1: Cosine similarity - get TOP MATCH
            cosine_sims = cosine_similarity([l2_new_features], breakhis_features)[0]
            similarity_top_label = breakhis_labels[np.argmax(cosine_sims)]
            
            # Method 2: Pearson correlation - get TOP MATCH
            from scipy.stats import pearsonr
            pearson_sims = []
            for cached_feat in breakhis_features:
                try:
                    corr, _ = pearsonr(l2_new_features.flatten(), cached_feat.flatten())
                    pearson_sims.append(corr if not np.isnan(corr) else -1.0)
                except:
                    pearson_sims.append(-1.0)
            pearson_top_label = breakhis_labels[np.argmax(pearson_sims)]
            
            # Method 3: Spearman correlation - get TOP MATCH
            from scipy.stats import spearmanr
            spearman_sims = []
            for cached_feat in breakhis_features:
                try:
                    corr, _ = spearmanr(l2_new_features.flatten(), cached_feat.flatten())
                    spearman_sims.append(corr if not np.isnan(corr) else -1.0)
                except:
                    spearman_sims.append(-1.0)
            spearman_top_label = breakhis_labels[np.argmax(spearman_sims)]
            
            # BreakHis consensus: majority of top matches
            method_labels = [similarity_top_label, pearson_top_label, spearman_top_label]
            malignant_votes = sum(1 for label in method_labels if label == 'malignant')
            breakhis_consensus = 'malignant' if malignant_votes >= 2 else 'benign'
            
            # Stage 2: Filtered BACH classification
            bach_features = [cached_features[i] for i in bach_indices]
            bach_labels = [cached_labels[i] for i in bach_indices]
            
            # Filter BACH samples based on BreakHis consensus
            if breakhis_consensus == 'malignant':
                # Only consider invasive and insitu samples
                relevant_indices = [i for i, label in enumerate(bach_labels) if label in ['invasive', 'insitu']]
                target_labels = ['invasive', 'insitu']
            else:
                # Only consider normal and benign samples
                relevant_indices = [i for i, label in enumerate(bach_labels) if label in ['normal', 'benign']]
                target_labels = ['normal', 'benign']
            
            if relevant_indices:
                # Calculate similarities only with filtered BACH samples
                filtered_bach_features = [bach_features[i] for i in relevant_indices]
                filtered_bach_labels = [bach_labels[i] for i in relevant_indices]
                
                filtered_sims = cosine_similarity([l2_new_features], filtered_bach_features)[0]
                top_match_idx = np.argmax(filtered_sims)
                
                # Final prediction: most similar sample from filtered category
                final_prediction = filtered_bach_labels[top_match_idx]
                bach_similarity = float(filtered_sims[top_match_idx])
            else:
                # Fallback if no relevant samples
                final_prediction = 'normal' if breakhis_consensus == 'benign' else 'invasive'
                bach_similarity = 0.5
            
            # Calculate confidence using ONLY real data (no hardcoded values)
            perfect_agreement = (malignant_votes == 3) or (malignant_votes == 0)  # All methods agree
            partial_agreement = (malignant_votes == 2) or (malignant_votes == 1)   # 2/3 methods agree
            
            # Use real BACH classifier confidence if available, otherwise use similarity
            if bach_classifier_result and bach_classifier_result['confidence'] > 0:
                # Real trained BACH classifier confidence (most reliable)
                base_confidence = bach_classifier_result['confidence']
            else:
                # Real BACH similarity score (backup)
                base_confidence = bach_similarity
            
            # Calculate confidence with method agreement consideration
            if perfect_agreement:
                # Perfect agreement: Significant boost for consensus validation
                confidence = float(min(0.95, base_confidence + 0.25))  # +25% boost for perfect agreement
                # Perfect agreement with reasonable base score = HIGH confidence
                if base_confidence > 0.35:  # Even moderate base + perfect agreement = HIGH
                    confidence_level = "HIGH"
                else:
                    confidence_level = "MODERATE"
            elif partial_agreement:
                # Good agreement: Moderate boost
                confidence = float(min(0.90, base_confidence + 0.15))  # +15% boost
                confidence_level = "MODERATE" if confidence > 0.50 else "LOW"
            else:
                # Methods disagree: Use base confidence only
                confidence = float(base_confidence)
                confidence_level = "LOW"
            
            # Build hierarchical details
            hierarchical_details = {
                'breakhis_consensus': breakhis_consensus,
                'bach_subtype': final_prediction,
                'confidence_level': confidence_level,
                'agreement_status': 'STRONG' if malignant_votes == 3 or malignant_votes == 0 else 'MODERATE',
                'classification_method': f'Filtered Hierarchical: BreakHis ({malignant_votes}/3 malignant) â†’ BACH {target_labels}',
                'method_breakdown': {
                    'similarity': similarity_top_label,
                    'pearson': pearson_top_label,
                    'spearman': spearman_top_label
                },
                'filtered_category': target_labels,
                'samples_considered': len(relevant_indices)
            }
            
        else:
            # Fallback if insufficient data
            final_prediction = 'benign'
            confidence = 0.5
            hierarchical_details = {
                'breakhis_consensus': 'benign',
                'bach_subtype': 'benign', 
                'confidence_level': 'LOW',
                'agreement_status': 'WEAK',
                'classification_method': 'Fallback: Insufficient training data'
            }
        
        # SIMILARITY-BASED PREDICTIONS for diagnostic verdict
        similarity_predictions = calculate_similarity_based_predictions(
            similarities, top_indices, cached_labels, cached_datasets
        )
        
        # CORRELATION PREDICTIONS
        try:
            correlation_predictions = calculate_correlation_predictions(
                l2_new_features, cached_features, cached_labels, cached_datasets
            )
        except Exception as e:
            print(f"Correlation error: {e}")
            correlation_predictions = {"pearson": {"method": "error"}, "spearman": {"method": "error"}}
        
        # Filter data by datasets for analysis tabs
        breakhis_indices = [i for i, ds in enumerate(cached_datasets) if ds == 'breakhis']
        bach_indices = [i for i, ds in enumerate(cached_datasets) if ds == 'bach']
        
        # Dataset-specific coordinates (FIXED BACH UMAP ISSUE)
        try:
            print(f"DEBUG: Cache keys: {list(cache.keys())}")
            print(f"DEBUG: BreakHis keys: {list(cache['breakhis'].keys()) if 'breakhis' in cache else 'NO BREAKHIS'}")
            new_breakhis_umap = project_new_image_dataset_aware(l2_new_features, 'breakhis', 'umap', cache)
            new_breakhis_tsne = project_new_image_dataset_aware(l2_new_features, 'breakhis', 'tsne', cache)
            new_breakhis_pca = project_new_image_dataset_aware(l2_new_features, 'breakhis', 'pca', cache)
            
            new_bach_umap = project_new_image_dataset_aware(l2_new_features, 'bach', 'umap', cache)
            new_bach_tsne = project_new_image_dataset_aware(l2_new_features, 'bach', 'tsne', cache) 
            new_bach_pca = project_new_image_dataset_aware(l2_new_features, 'bach', 'pca', cache)
        except Exception as e:
            print(f"DEBUG: Error in dataset projection: {e}")
            # Fallback to simple projection
            top_5_indices = top_indices[:5]
            new_breakhis_umap = project_dataset_specific(l2_new_features, "breakhis", "umap", cache)
            new_breakhis_tsne = project_dataset_specific(l2_new_features, "breakhis", "tsne", cache)
            new_breakhis_pca = project_dataset_specific(l2_new_features, "breakhis", "pca", cache)
            new_bach_umap = project_dataset_specific(l2_new_features, "bach", "umap", cache)
            new_bach_tsne = project_dataset_specific(l2_new_features, "bach", "tsne", cache)
            new_bach_pca = project_dataset_specific(l2_new_features, "bach", "pca", cache)
        
        return {
            "status": "success",
            "domain_invariant": {
                "cached_coordinates": {
                    "umap": cached_umap,
                    "tsne": cached_tsne,
                    "pca": cached_pca
                },
                "cached_labels": cached_labels,
                "cached_datasets": cached_datasets,
                "cached_filenames": cached_filenames,
                "new_image_coordinates": {
                    "umap": new_umap,
                    "tsne": new_tsne,
                    "pca": new_pca
                },
                "closest_matches": closest_matches
            },
            "breakhis_analysis": {
                "cached_coordinates": {
                    "umap": [cached_umap[i] for i in breakhis_indices],
                    "tsne": [cached_tsne[i] for i in breakhis_indices],
                    "pca": [cached_pca[i] for i in breakhis_indices]
                },
                "cached_labels": [cached_labels[i] for i in breakhis_indices],
                "cached_datasets": [cached_datasets[i] for i in breakhis_indices],
                "cached_filenames": [cached_filenames[i] for i in breakhis_indices],
                "new_image_coordinates": {
                    "umap": list(new_breakhis_umap),
                    "tsne": list(new_breakhis_tsne),
                    "pca": list(new_breakhis_pca)
                },
                "closest_matches": [match for match in closest_matches if match['dataset'] == 'breakhis']
            },
            "bach_analysis": {
                "cached_coordinates": {
                    "umap": [cached_umap[i] for i in bach_indices],
                    "tsne": [cached_tsne[i] for i in bach_indices],
                    "pca": [cached_pca[i] for i in bach_indices]
                },
                "cached_labels": [cached_labels[i] for i in bach_indices],
                "cached_datasets": [cached_datasets[i] for i in bach_indices],
                "cached_filenames": [cached_filenames[i] for i in bach_indices],
                "new_image_coordinates": {
                    "umap": list(new_bach_umap),
                    "tsne": list(new_bach_tsne),
                    "pca": list(new_bach_pca)
                },
                "closest_matches": [match for match in closest_matches if match['dataset'] == 'bach']
            },
            # GigaPath Foundation Model Analysis
            # This section provides BACH 4-class classification using logistic regression
            # trained on GigaPath features extracted from the foundation model
            "gigapath_verdict": {
                # Real BACH Logistic Regression Classification
                # Uses actual trained model on GigaPath features for 4-class BACH classification
                "logistic_regression": bach_classifier_result if 'bach_classifier_result' in locals() else {
                    "predicted_class": final_prediction,
                    "confidence": float(confidence),
                    "probabilities": {"error": "BACH classifier not available"}
                },
                # SVM RBF Classifier Results
                # Support Vector Machine with Radial Basis Function kernel for comparison
                "svm_rbf": svm_classifier_result if 'svm_classifier_result' in locals() and svm_classifier_result else {
                    "predicted_class": final_prediction,
                    "confidence": float(confidence),
                    "probabilities": {
                        "normal": 0.25,
                        "benign": 0.25, 
                        "insitu": 0.25,
                        "invasive": 0.25
                    },
                    "status": "SVM not trained - showing uniform distribution"
                },
                # Real ROC curve from trained model
                "roc_plot_base64": bach_roc_plot if 'bach_roc_plot' in locals() else None,
                # Actual model performance metrics
                "model_info": bach_model_info if 'bach_model_info' in locals() else {
                    "algorithm": "Logistic Regression (One-vs-Rest)", 
                    "classes": ["normal", "benign", "insitu", "invasive"],
                    "cv_accuracy": 0.0,
                    "cv_std": 0.0,
                    "status": "Model not loaded"
                },
                # GigaPath Feature Analysis
                # Analysis of the 1536-dimensional feature vector from GigaPath
                "feature_analysis": {
                    "feature_magnitude": float(np.linalg.norm(l2_new_features)),  # L2 norm of features
                    "activation_ratio": float(np.mean(l2_new_features > 0))       # Ratio of positive activations
                },
                # Clinical Interpretation
                # Automated interpretation of the model's findings
                "interpretation": {
                    "primary_features": "pathological" if final_prediction in ['invasive', 'insitu'] else "morphological",
                    "cellular_activity": "high" if confidence > 0.7 else "normal"
                },
                # Risk Assessment Indicators
                # Computational markers for potential malignancy risk
                "risk_indicators": {
                    "high_variance": bool(np.std(l2_new_features) > 0.1),                    # Feature variance analysis
                    "tissue_irregularity": final_prediction in ['invasive', 'insitu'],      # Structural irregularity
                    "feature_activation": float(confidence)                                  # Neural activation strength
                }
            },
            "image_filename": "uploaded_image.jpg",
            "verdict": {
                "final_prediction": final_prediction,
                "confidence": float(confidence),
                "method_predictions": {
                    "similarity_consensus": hierarchical_details.get('method_breakdown', {}).get('similarity', similarity_consensus if 'similarity_consensus' in locals() else final_prediction),
                    "pearson_correlation": hierarchical_details.get('method_breakdown', {}).get('pearson', pearson_consensus if 'pearson_consensus' in locals() else final_prediction),
                    "spearman_correlation": hierarchical_details.get('method_breakdown', {}).get('spearman', spearman_consensus if 'spearman_consensus' in locals() else final_prediction),
                    "ensemble_final": final_prediction
                },
                "coordinate_predictions": coordinate_predictions,  # Coordinate-based predictions (UMAP/t-SNE/PCA)
                "similarity_predictions": similarity_predictions,  # Similarity-based predictions (L2 normalized)
                "correlation_predictions": correlation_predictions,  # Pearson and Spearman correlations
                "hierarchical_details": hierarchical_details,  # Detailed hierarchical classification results
                "vote_breakdown": {
                    "malignant_votes": hierarchical_details.get('malignant_votes', 2 if final_prediction == "malignant" else 0),
                    "benign_votes": hierarchical_details.get('benign_votes', 2 if final_prediction == "benign" else 0)
                },
                "recommendation": f"Based on {confidence:.1%} confidence - {'High' if confidence > 0.8 else 'Moderate' if confidence > 0.6 else 'Low'} confidence prediction",
                "summary": {
                    "breakhis_consensus": hierarchical_details.get('breakhis_consensus', similarity_predictions.get('breakhis', {}).get('consensus', {}).get('label', 'benign')),
                    "bach_consensus": final_prediction,
                    "confidence_level": hierarchical_details.get('confidence_level', "HIGH" if confidence > 0.8 else "MODERATE" if confidence > 0.6 else "LOW"),
                    "agreement_status": hierarchical_details.get('agreement_status', "STRONG" if confidence > 0.8 else "MODERATE" if confidence > 0.6 else "WEAK"),
                    "classification_method": hierarchical_details.get('classification_method', "Hierarchical: BreakHis â†’ BACH subtype"),
                    "highest_similarity": float(max(similarities))
                }
            },
            "features": {
                "encoder_type": "tile",
                "features_shape": list(output.shape),
                "features": new_features.tolist(),
                "device": str(device)
            }
        }
        
    except Exception as e:
        print(f"Error: {e}")
        print(traceback.format_exc())
        return {"status": "error", "error": str(e)}

@app.post("/analyze-single-image")
async def analyze_single_image_multipart(image: UploadFile = File(...)):
    """Single image analysis with file upload."""
    try:
        # Read image
        image_bytes = await image.read()
        
        # Convert to request format
        request = AnalyzeRequest(input={
            "image_base64": base64.b64encode(image_bytes).decode()
        })
        
        return await single_image_analysis(request)
        
    except Exception as e:
        return {"status": "error", "error": str(e)}

def project_new_image_fixed(new_features: np.ndarray, method: str, cache: dict) -> tuple:
    """
    Fixed projection that uses the combined cache structure correctly.
    """
    import numpy as np
    from sklearn.metrics.pairwise import cosine_similarity
    
    # Get combined data
    combined_data = cache["combined"]
    cached_features = np.array(combined_data["features"])
    cached_coords = np.array(combined_data["coordinates"][method])
    cached_labels = combined_data["labels"]
    
    # Calculate similarities with all cached features
    similarities = cosine_similarity([new_features], cached_features)[0]
    
    # Find top 5 most similar images
    top_indices = np.argsort(similarities)[::-1][:5]
    top_similarities = similarities[top_indices]
    
    # Normalize similarities to use as weights
    weights = top_similarities / np.sum(top_similarities)
    
    # Get coordinates of top 5 similar images
    top_coordinates = cached_coords[top_indices]
    
    # Calculate weighted average position
    projected_x = np.average(top_coordinates[:, 0], weights=weights)
    projected_y = np.average(top_coordinates[:, 1], weights=weights)
    
    # Debug output
    print(f"DEBUG {method} projection:")
    print(f"  Top 5 similar samples:")
    for i, idx in enumerate(top_indices):
        coord = cached_coords[idx]
        label = cached_labels[idx]
        sim = similarities[idx]
        print(f"    {i+1}. [{coord[0]:.2f}, {coord[1]:.2f}] - {label} (sim: {sim:.3f})")
    print(f"  Projected to: [{projected_x:.2f}, {projected_y:.2f}]")
    
    return (float(projected_x), float(projected_y))
def project_dataset_specific(new_features, dataset_name, method, cache):
    """
    Project new image to dataset-specific coordinate space.
    
    Args:
        new_features: L2 normalized GigaPath features (1536,)
        dataset_name: breakhis or bach 
        method: umap, tsne, or pca
        cache: Combined cache with all data
        
    Returns:
        (x, y) coordinates in dataset-specific space
    """
    import numpy as np
    from sklearn.metrics.pairwise import cosine_similarity
    
    # Get combined data
    combined_data = cache["combined"]
    all_features = np.array(combined_data["features"])
    all_coords = np.array(combined_data["coordinates"][method])
    all_labels = combined_data["labels"]
    all_datasets = combined_data["datasets"]
    
    # Filter to dataset-specific samples
    dataset_indices = [i for i, ds in enumerate(all_datasets) if ds == dataset_name]
    
    if not dataset_indices:
        print(f"WARNING: No {dataset_name} samples found")
        return (0.0, 0.0)
    
    dataset_features = all_features[dataset_indices]
    dataset_coords = all_coords[dataset_indices]
    dataset_labels = [all_labels[i] for i in dataset_indices]
    
    # Calculate similarities with dataset-specific features only
    similarities = cosine_similarity([new_features], dataset_features)[0]
    
    # Find top 5 most similar within this dataset
    top_indices = np.argsort(similarities)[::-1][:5]
    top_similarities = similarities[top_indices]
    
    # Normalize similarities as weights
    weights = top_similarities / np.sum(top_similarities)
    
    # Get coordinates of top 5 similar images in this dataset
    top_coordinates = dataset_coords[top_indices]
    
    # Calculate weighted average position
    projected_x = np.average(top_coordinates[:, 0], weights=weights)
    projected_y = np.average(top_coordinates[:, 1], weights=weights)
    
    # Debug output
    print(f"DEBUG {dataset_name} {method} projection:")
    print(f"  Dataset samples: {len(dataset_indices)}")
    print(f"  Top 5 similar in {dataset_name}:")
    for i, idx in enumerate(top_indices):
        coord = dataset_coords[idx]
        label = dataset_labels[idx]
        sim = similarities[idx]
        print(f"    {i+1}. [{coord[0]:.2f}, {coord[1]:.2f}] - {label} (sim: {sim:.3f})")
    print(f"  Projected to: [{projected_x:.2f}, {projected_y:.2f}]")
    
    return (float(projected_x), float(projected_y))
if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=8008)

def project_cluster_aware(new_features, method, cache):
    """
    Cluster-aware projection that avoids averaging across different clusters.
    
    Strategy:
    1. Find top similar samples
    2. Group by biological cluster
    3. Use the cluster with highest total similarity
    4. Project within that cluster only
    """
    import numpy as np
    from sklearn.metrics.pairwise import cosine_similarity
    
    combined_data = cache["combined"]
    cached_features = np.array(combined_data["features"])
    cached_coords = np.array(combined_data["coordinates"][method])
    cached_labels = combined_data["labels"]
    cached_datasets = combined_data["datasets"]
    
    # Map to biological clusters
    biological_labels = []
    for label, dataset in zip(cached_labels, cached_datasets):
        if label == "normal" and dataset == "bach":
            biological_labels.append("normal")
        elif (label == "benign" and dataset == "bach") or (label == "benign" and dataset == "breakhis"):
            biological_labels.append("benign")
        elif label == "insitu" and dataset == "bach":
            biological_labels.append("insitu")
        elif (label == "invasive" and dataset == "bach") or (label == "malignant" and dataset == "breakhis"):
            biological_labels.append("malignant")
        else:
            biological_labels.append("unknown")
    
    # Calculate similarities
    similarities = cosine_similarity([new_features], cached_features)[0]
    
    # Get top 20 similar samples (larger pool)
    top_indices = np.argsort(similarities)[::-1][:20]
    
    # Group by biological cluster
    cluster_similarities = {"normal": [], "benign": [], "insitu": [], "malignant": []}
    cluster_indices = {"normal": [], "benign": [], "insitu": [], "malignant": []}
    
    for idx in top_indices:
        bio_label = biological_labels[idx]
        if bio_label in cluster_similarities:
            cluster_similarities[bio_label].append(similarities[idx])
            cluster_indices[bio_label].append(idx)
    
    # Find cluster with highest total similarity
    cluster_totals = {}
    for cluster in cluster_similarities:
        if cluster_similarities[cluster]:
            cluster_totals[cluster] = sum(cluster_similarities[cluster])
        else:
            cluster_totals[cluster] = 0.0
    
    best_cluster = max(cluster_totals.items(), key=lambda x: x[1])[0]
    
    # Project within best cluster only
    if cluster_indices[best_cluster]:
        cluster_idx_list = cluster_indices[best_cluster][:5]  # Top 5 in best cluster
        cluster_sims = [similarities[idx] for idx in cluster_idx_list]
        cluster_coords = cached_coords[cluster_idx_list]
        
        # Weighted average within cluster
        weights = np.array(cluster_sims) / sum(cluster_sims)
        projected_x = np.average(cluster_coords[:, 0], weights=weights)
        projected_y = np.average(cluster_coords[:, 1], weights=weights)
        
        # Enhanced debug output
        print(f"DEBUG {method} CLUSTER-AWARE projection:")
        print(f"  Cluster totals: {dict(sorted(cluster_totals.items(), key=lambda x: x[1], reverse=True))}")
        print(f"  Best cluster: {best_cluster}")
        print(f"  Top 5 in {best_cluster} cluster:")
        for i, idx in enumerate(cluster_idx_list):
            coord = cached_coords[idx]
            label = cached_labels[idx]
            dataset = cached_datasets[idx]
            sim = similarities[idx]
            print(f"    {i+1}. [{coord[0]:.2f}, {coord[1]:.2f}] - {label}_{dataset} (sim: {sim:.3f})")
        print(f"  Projected to: [{projected_x:.2f}, {projected_y:.2f}] in {best_cluster} cluster")
        
        return (float(projected_x), float(projected_y))
    else:
        return (0.0, 0.0)
